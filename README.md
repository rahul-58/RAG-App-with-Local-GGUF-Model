# ğŸ“„ RAG PDF QA App with Local GGUF Model

This project is a **Retrieval-Augmented Generation (RAG)** application that allows users to upload one or more PDF files, ask questions, and receive AI-generated answers using a **local LLM in `.gguf` format** (via `llama.cpp`). It runs entirely offline and uses fast local embeddings and a vector store.

## ğŸš€ Features

- Upload and parse multiple PDF files
- Split documents into overlapping chunks for context retention
- Embed and index using `MiniLM` + FAISS
- Query documents with natural language questions
- Generate answers using a local LLM (`.gguf`) with `llama-cpp-python`
- Fully private & local â€” no external APIs required

## ğŸ§  How It Works

1. PDFs â†’ Text â†’ Chunks
2. Chunks â†’ Embeddings â†’ FAISS index
3. Question â†’ Embedding â†’ Similarity Search
4. Top chunks + question â†’ Prompt to LLM
5. Local `.gguf` model generates answer

## ğŸ›  Tech Stack

- **Frontend**: Streamlit
- **Embedding**: Sentence-Transformers (`all-MiniLM-L6-v2`)
- **Vector Store**: FAISS
- **LLM Inference**: `llama.cpp` via `llama-cpp-python`
- **PDF Parsing**: PyMuPDF

## ğŸ“ Folder Structure

rag_gguf_pdf_qa/
â”œâ”€â”€ app.py # Streamlit frontend
â”œâ”€â”€ rag_engine.py # RAG logic (embedding, retrieval, LLM)
â”œâ”€â”€ utils.py # PDF parsing and text chunking
â”œâ”€â”€ models/
â”‚ â””â”€â”€ your-model.gguf # Downloaded LLM (e.g., Mistral, Zephyr)
â””â”€â”€ requirements.txt # Python dependencies


## ğŸ”§ Setup Instructions

1. **Clone the repo**

git clone https://github.com/your-username/rag-gguf-pdf-qa.git
cd rag-gguf-pdf-qa

2. **Set up virtual environment**
python -m venv venv
source venv/bin/activate  # or venv\\Scripts\\activate

3. **Install dependencies**
pip install -r requirements.txt

4. **Download a GGUF model**
Choose from TheBloke models on Hugging Face, e.g.:

Mistral-7B-Instruct-v0.2.Q4_K_M.gguf

zephyr-7B-beta.Q4_K_M.gguf

Place it in the models/ folder.

5. **Run the app**
streamlit run app.py