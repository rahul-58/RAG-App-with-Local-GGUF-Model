# ğŸ“„ RAG PDF QA App with Local GGUF Model

This project is a **Retrieval-Augmented Generation (RAG)** application that allows users to upload one or more PDF files, ask questions, and receive **AI-generated  context-aware answers** using a **local LLM in `.gguf` format** (via `llama.cpp`). Built with **Streamlit**, it provides an interactive web UI with complete privacy â€” **no external APIs or cloud LLMs used**.

## ğŸš€ Features

- Upload and parse multiple PDF files
- Split documents into overlapping chunks for context retention
- Embed and index using `MiniLM` + FAISS
- Query documents with natural language questions
- Generate answers using a local LLM (`.gguf`) with `llama-cpp-python`
- Fully private & local â€” no external APIs required

## ğŸ§  How It Works

1. PDFs â†’ Text â†’ Chunks
2. Chunks â†’ Embeddings â†’ FAISS index
3. Question â†’ Embedding â†’ Similarity Search
4. Top chunks + question â†’ Prompt to LLM
5. Local `.gguf` model generates answer

## ğŸ›  Tech Stack

- **Frontend**: Streamlit
- **Embedding**: Sentence-Transformers (`all-MiniLM-L6-v2`)
- **Vector Store**: FAISS
- **LLM Inference**: `llama.cpp` via `llama-cpp-python`
- **PDF Parsing**: PyMuPDF

## ğŸ“ Folder Structure

```
rag_gguf_pdf_qa/
â”œâ”€â”€ app.py                      # Streamlit frontend
â”œâ”€â”€ rag_engine.py               # RAG logic (embedding, retrieval, LLM)
â”œâ”€â”€ utils.py                    # PDF parsing and text chunking
â”œâ”€â”€ models/
â”‚ â””â”€â”€ your-model.gguf           # Downloaded LLM (e.g., Mistral, Zephyr)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md
```


## ğŸ”§ Setup Instructions

1. **Clone the repo**

```bash
git clone https://github.com/rahul-58/RAG-App-with-Local-GGUF-Model.git
cd RAG-App-with-Local-GGUF-Model
```

2. **Set up virtual environment**

```bash
python -m venv venv
source venv/bin/activate  # or venv\\Scripts\\activate
```

3. **Install dependencies**

```bash
pip install -r requirements.txt
```

4. **Download a GGUF model**

```
Choose any model on Hugging Face, e.g.:

Mistral-7B-Instruct-v0.2.Q4_K_M.gguf

zephyr-7B-beta.Q4_K_M.gguf

Place it in the models/ folder.
```

5. **Run the app**

```bash
streamlit run app.py
```

Open [http://localhost:8501] in your browser.

---

## ğŸ“Œ Example Use Case

1. Upload your course notes or technical research papers as PDFs  
2. Ask: *"What are the main contributions of this paper?"* or *"Explain the algorithm used in section 3."*  
3. The app retrieves the most relevant chunks and generates an answer using the local model

---
